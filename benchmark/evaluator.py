"""
è½»é‡åŒ–Benchmarkè¯„ä¼°å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰
è¯„ä¼°3ä¸ªç»´åº¦ï¼šæ´å¯Ÿè´¨é‡ã€æ¨ç†è¿‡ç¨‹ã€æ•ˆç‡
ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œæ´å¯ŸåŒ¹é…

æ”¹è¿›ç‚¹ï¼š
1. PrecisionçœŸå®è®¡ç®—ï¼ˆä¸å†å‡è®¾æ‰€æœ‰æ´å¯Ÿéƒ½æœ‰æ•ˆï¼‰
2. DepthçœŸå®è¯„ä¼°ï¼ˆåŸºäºå…³é”®è¯æ£€æµ‹æ´å¯Ÿå±‚æ¬¡ï¼‰
3. å†—ä½™æ£€æµ‹ç²¾ç¡®åŒ–ï¼ˆåŸºäºçŠ¶æ€æŒ‡çº¹è€Œéè¿ç»­é‡å¤ï¼‰
4. é‡Œç¨‹ç¢‘æ£€æŸ¥å¢å¼ºï¼ˆåŒæ—¶æ£€æŸ¥å·¥å…·+æ´å¯Ÿï¼‰
5. æ¨ç†è¿è´¯æ€§è¯„ä¼°ï¼ˆæ–°å¢ï¼‰
"""

import json
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer


class BenchmarkEvaluator:
    """è½»é‡åŒ–benchmarkè¯„ä¼°å™¨ï¼ˆä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
    
    è¯„ä¼°3ä¸ªç»´åº¦ï¼š
    1. æ´å¯Ÿè´¨é‡ (50%): å‘ç°çš„æ´å¯Ÿæ˜¯å¦å‡†ç¡®ã€æ·±å…¥ã€å®Œæ•´
    2. æ¨ç†è¿‡ç¨‹ (30%): æ€ç»´é“¾æ˜¯å¦åˆç†ã€æ˜¯å¦è¾¾åˆ°å…³é”®é‡Œç¨‹ç¢‘
    3. æ•ˆç‡ (20%): æ­¥éª¤æ˜¯å¦å†—ä½™ã€æ˜¯å¦å¿«é€Ÿæ”¶æ•›
    """
    
    def __init__(self, ground_truth: Dict):
        self.gt = ground_truth
        # åŠ è½½å¤šè¯­è¨€è¯­ä¹‰æ¨¡å‹
        print("ğŸ“¦ åŠ è½½è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹...")
        self.semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def evaluate(self, agent_result: Dict) -> Dict:
        """
        å®Œæ•´è¯„ä¼°
        
        Args:
            agent_result: ç³»ç»Ÿè¾“å‡ºçš„ç»“æœï¼ˆæ¥è‡ªexploration JSONï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœï¼ŒåŒ…å«2ä¸ªç»´åº¦çš„åˆ†æ•°å’Œæ€»åˆ†
        """
        # æå–agentçš„æ¢ç´¢æ­¥éª¤å’Œæ´å¯Ÿ
        explorations = agent_result.get('explorations', [])
        
        # 1. æ´å¯Ÿè´¨é‡è¯„ä¼°
        insight_score = self.evaluate_insight_quality(explorations)
        
        # 2. æ¨ç†è¿‡ç¨‹è¯„ä¼°
        reasoning_score = self.evaluate_reasoning_process(explorations)
        
        # åŠ æƒæ€»åˆ†
        weights = {
            'insight_quality': 0.60,
            'reasoning_process': 0.40
        }
        
        total_score = (
            insight_score * weights['insight_quality'] +
            reasoning_score * weights['reasoning_process']
        )
        
        return {
            'total_score': round(total_score, 2),
            'dimension_scores': {
                'insight_quality': round(insight_score, 2),
                'reasoning_process': round(reasoning_score, 2)
            },
            'weights': weights,
            'details': {
                'total_explorations': len(explorations),
                'insights_found': self._count_insights_found(explorations),
                'tools_used': self._get_tools_used(explorations)
            }
        }
    
    def evaluate_insight_quality(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°ç»´åº¦1: æ´å¯Ÿè´¨é‡
        
        æ”¹è¿›ç‚¹ï¼š
        1. PrecisionçœŸå®è®¡ç®—
        2. DepthçœŸå®è¯„ä¼°ï¼ˆåŸºäºå…³é”®è¯æ£€æµ‹å±‚æ¬¡ï¼‰
        3. æ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼ˆ0.5-1.0åˆ†ï¼‰
        """
        gt_insights = self.gt['insight_quality']['critical_insights']
        criteria = self.gt['insight_quality']['evaluation_criteria']
        
        # æ”¶é›†æ‰€æœ‰agentå‘ç°çš„æ´å¯Ÿ
        agent_insights = []
        for exp in explorations:
            summary = exp.get('analysis_summary', {})
            agent_insights.extend(summary.get('key_insights', []))
            agent_insights.extend(summary.get('patterns_found', []))
        
        if not agent_insights:
            return 0.0
        
        # === Recall - æ”¯æŒéƒ¨åˆ†åŒ¹é… ===
        recall_scores = []
        for gt_insight in gt_insights:
            match_score = self._calculate_insight_match_score(gt_insight, agent_insights)
            recall_scores.append(match_score)
        
        recall = np.mean(recall_scores) if recall_scores else 0
        
        # === æ”¹è¿›2: Precision - çœŸå®è®¡ç®— ===
        # æ£€æŸ¥æ¯ä¸ªagentæ´å¯Ÿæ˜¯å¦åŒ¹é…ä»»ä¸€GTæ´å¯Ÿ
        matched_agent_count = 0
        for agent_insight in agent_insights:
            if self._is_valid_insight(agent_insight, gt_insights):
                matched_agent_count += 1
        
        precision = matched_agent_count / len(agent_insights) if agent_insights else 0
        
        # === æ”¹è¿›3: Depth - çœŸå®è¯„ä¼°æ´å¯Ÿå±‚æ¬¡ ===
        depth_scores = [self._assess_insight_depth(ins) for ins in agent_insights]
        avg_depth = np.mean(depth_scores) / 3.0 if depth_scores else 0  # å½’ä¸€åŒ–åˆ°0-1
        
        # åŠ æƒè®¡ç®—
        score = (
            recall * criteria['recall_weight'] * 100 +
            precision * criteria['precision_weight'] * 100 +
            avg_depth * criteria['depth_weight'] * 100
        )
        
        return min(100, score)
    
    def evaluate_reasoning_process(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°ç»´åº¦2: æ¨ç†è¿‡ç¨‹
        
        åŒ…å«ä¸‰ä¸ªå­ç»´åº¦ï¼š
        1. æ¨ç†è¿è´¯æ€§ (20%)
        2. å·¥å…·è°ƒç”¨è¯„ä¼° (40%)
        3. å·¥å…·è·¯å¾„è¯„ä¼° (40%)
        """
        # === å­ç»´åº¦1: æ¨ç†è¿è´¯æ€§ ===
        coherence_score = self._evaluate_reasoning_coherence(explorations)
        
        # === å­ç»´åº¦2: å·¥å…·è°ƒç”¨è¯„ä¼° ===
        tool_usage_score = self._evaluate_tool_usage(explorations)
        
        # === å­ç»´åº¦3: å·¥å…·è·¯å¾„è¯„ä¼° ===
        tool_path_score = self._evaluate_tool_path(explorations)
        
        # ç»¼åˆï¼ˆ20% è¿è´¯æ€§ + 40% å·¥å…·è°ƒç”¨ + 40% å·¥å…·è·¯å¾„ï¼‰
        return coherence_score * 0.2 + tool_usage_score * 0.4 + tool_path_score * 0.4
    
    # ========================================
    #è®¡ç®—æ–¹æ³•
    # ========================================
    
    def _calculate_insight_match_score(
        self, 
        gt_insight: Dict, 
        agent_insights: List[str]
    ) -> float:
        """è®¡ç®—GTæ´å¯Ÿä¸agentæ´å¯Ÿçš„æœ€ä½³åŒ¹é…åˆ†æ•°
        
        è¿”å›ï¼š0.0-1.0ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè¿ç»­å€¼ï¼‰
        """
        gt_content = gt_insight['content']
        gt_embedding = self.semantic_model.encode(gt_content, convert_to_numpy=True)
        
        max_similarity = 0.0
        for agent_insight in agent_insights:
            if not agent_insight or len(agent_insight.strip()) < 5:
                continue
            
            agent_embedding = self.semantic_model.encode(agent_insight, convert_to_numpy=True)
            similarity = np.dot(gt_embedding, agent_embedding) / (
                np.linalg.norm(gt_embedding) * np.linalg.norm(agent_embedding) + 1e-8
            )
            max_similarity = max(max_similarity, similarity)
        
        return max_similarity
    
    def _is_valid_insight(self, agent_insight: str, gt_insights: List[Dict]) -> bool:
        """æ£€æŸ¥agentæ´å¯Ÿæ˜¯å¦åŒ¹é…ä»»ä¸€GTæ´å¯Ÿï¼ˆPrecision"""
        if not agent_insight or len(agent_insight.strip()) < 5:
            return False
        
        agent_embedding = self.semantic_model.encode(agent_insight, convert_to_numpy=True)
        
        for gt_insight in gt_insights:
            gt_content = gt_insight['content']
            gt_embedding = self.semantic_model.encode(gt_content, convert_to_numpy=True)
            
            similarity = np.dot(agent_embedding, gt_embedding) / (
                np.linalg.norm(agent_embedding) * np.linalg.norm(gt_embedding) + 1e-8
            )
            
            # é˜ˆå€¼ä¸º0.5ï¼ŒPrecisionå…³æ³¨"æ˜¯å¦çœŸå®/ç›¸å…³"
            if similarity > 0.5:
                return True
        
        return False
    
    def _assess_insight_depth(self, insight: str) -> int:
        """è¯„ä¼°æ´å¯Ÿæ·±åº¦å±‚æ¬¡ï¼ˆæ˜¯å¦è€ƒè™‘ç”¨nlpåˆ†ç±»å™¨ï¼‰
        
        Level 3 (é¢„æµ‹æ€§): "å¦‚æœ...å°†ä¼š..."ã€"é¢„æµ‹"ã€"é¢„æœŸ"
        Level 2 (è¯Šæ–­æ€§): "å› ä¸º"ã€"ç”±äº"ã€"å¯¼è‡´"ã€"åŸå› æ˜¯"
        Level 1 (æè¿°æ€§): å…¶ä»–
        
        Returns:
            1-3ä¹‹é—´çš„æ•´æ•°
        """
        if not insight:
            return 1
        
        insight_lower = insight.lower()
        
        # Level 3å…³é”®è¯ï¼šé¢„æµ‹æ€§
        level3_keywords = [
            'é¢„æµ‹', 'é¢„æœŸ', 'å°†ä¼š', 'ä¼šå¯¼è‡´', 'é¢„è®¡', 
            'will', 'forecast', 'predict', 'expect',
            'å¦‚æœ', 'if', 'å‡è®¾', 'assume'
        ]
        
        # Level 2å…³é”®è¯ï¼šè¯Šæ–­æ€§
        level2_keywords = [
            'å› ä¸º', 'ç”±äº', 'å¯¼è‡´', 'åŸå› ', 'é€ æˆ',
            'because', 'due to', 'caused by', 'reason',
            'æ‰€ä»¥', 'therefore', 'thus', 'è¡¨æ˜', 'indicate'
        ]
        
        if any(kw in insight_lower for kw in level3_keywords):
            return 3
        elif any(kw in insight_lower for kw in level2_keywords):
            return 2
        else:
            return 1
    
    def _evaluate_tool_usage(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°å·¥å…·è°ƒç”¨å®Œæ•´æ€§
        
        æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†GTè¦æ±‚çš„æ‰€æœ‰å¿…éœ€å·¥å…·
        
        Returns:
            0-100ä¹‹é—´çš„åˆ†æ•°
        """
        if 'required_tools' not in self.gt['reasoning_process']:
            return 100.0  # å¦‚æœGTæ²¡æœ‰å®šä¹‰required_toolsï¼Œé»˜è®¤æ»¡åˆ†
        
        required_tools = set(self.gt['reasoning_process']['required_tools'])
        if not required_tools:
            return 100.0
        
        # ä»explorationsä¸­æå–å®é™…ä½¿ç”¨çš„å·¥å…·
        used_tools = set()
        for exp in explorations:
            tool_exec = exp.get('tool_execution') or {}
            tool_name = tool_exec.get('tool_name')
            if tool_name:
                used_tools.add(tool_name)
        
        # è®¡ç®—è¦†ç›–ç‡
        if not required_tools:
            return 100.0
        
        covered_tools = required_tools & used_tools
        coverage = len(covered_tools) / len(required_tools)
        
        return coverage * 100
    
    def _evaluate_tool_path(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°å·¥å…·è°ƒç”¨è·¯å¾„ç›¸ä¼¼åº¦
        
        æ¯”è¾ƒagentçš„å·¥å…·è°ƒç”¨åºåˆ—ä¸GT reference_optimal_pathçš„ç›¸ä¼¼åº¦
        ä½¿ç”¨æœ€é•¿å…¬å…±å­åºåˆ—(LCS)ç®—æ³•
        
        Returns:
            0-100ä¹‹é—´çš„åˆ†æ•°
        """
        if 'reference_optimal_path' not in self.gt['reasoning_process']:
            return 100.0  # å¦‚æœGTæ²¡æœ‰å®šä¹‰å‚è€ƒè·¯å¾„ï¼Œé»˜è®¤æ»¡åˆ†
        
        gt_path = self.gt['reasoning_process']['reference_optimal_path']
        if not gt_path:
            return 100.0
        
        # æå–GTå·¥å…·åºåˆ—
        gt_tool_sequence = [step['tool'] for step in gt_path]
        
        # æå–agentå·¥å…·åºåˆ—
        agent_tool_sequence = []
        for exp in explorations:
            tool_exec = exp.get('tool_execution') or {}
            tool_name = tool_exec.get('tool_name')
            if tool_name:
                agent_tool_sequence.append(tool_name)
        
        if not agent_tool_sequence:
            return 0.0
        
        # è®¡ç®—LCSé•¿åº¦
        lcs_length = self._longest_common_subsequence(gt_tool_sequence, agent_tool_sequence)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼šLCSé•¿åº¦ / GTåºåˆ—é•¿åº¦
        similarity = lcs_length / len(gt_tool_sequence) if gt_tool_sequence else 0
        
        return similarity * 100
    
    def _longest_common_subsequence(self, seq1: List[str], seq2: List[str]) -> int:
        """è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    def _evaluate_reasoning_coherence(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°æ¨ç†è¿è´¯æ€§ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
        
        æ£€æŸ¥ï¼š
        1. åç»­æ­¥éª¤æ˜¯å¦åŸºäºå‰é¢çš„å‘ç°
        2. æ˜¯å¦æœ‰é€»è¾‘è·³è·ƒ
        3. æ˜¯å¦æœ‰å‰åçŸ›ç›¾
        
        Returns:
            0-100ä¹‹é—´çš„åˆ†æ•°
        """
        if len(explorations) <= 1:
            return 100.0  # åªæœ‰1æ­¥ï¼Œæ— éœ€æ£€æŸ¥è¿è´¯æ€§
        
        coherence_score = 100.0
        
        for i in range(1, len(explorations)):
            current = explorations[i]
            previous = explorations[i-1]
            
            # è·å–å‰ä¸€æ­¥çš„æ´å¯Ÿ
            prev_summary = previous.get('analysis_summary', {})
            prev_insights = (
                prev_summary.get('key_insights', []) + 
                prev_summary.get('patterns_found', [])
            )
            
            # è·å–å½“å‰æ­¥çš„æ¨ç†ä¾æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            curr_reasoning = current.get('reasoning', '')
            curr_tool = (current.get('tool_execution') or {}).get('tool_name', '')
            
            # å¦‚æœå‰ä¸€æ­¥æœ‰é‡è¦å‘ç°ï¼Œä½†å½“å‰æ­¥å®Œå…¨å¿½ç•¥ï¼Œæ‰£åˆ†
            if prev_insights and curr_reasoning:
                has_reference = any(
                    self._is_concept_referenced(insight, curr_reasoning) 
                    for insight in prev_insights
                )
                
                if not has_reference:
                    coherence_score -= 5  # æœªå¼•ç”¨å‰é¢å‘ç°ï¼Œæ‰£5åˆ†
            
            # æ£€æŸ¥å·¥å…·ä½¿ç”¨çš„åˆç†æ€§
            # ä¾‹å¦‚ï¼šå·²ç»è¯†åˆ«äº†2ä¸ªç¾¤ä½“ï¼Œå´åˆè°ƒç”¨identify_clusters
            if i > 0 and curr_tool == 'identify_clusters':
                # æ£€æŸ¥ä¹‹å‰æ˜¯å¦å·²ç»è¯†åˆ«è¿‡èšç±»
                has_identified_before = any(
                    (exp.get('tool_execution') or {}).get('tool_name') == 'identify_clusters'
                    for exp in explorations[:i]
                )
                if has_identified_before:
                    coherence_score -= 10  # é‡å¤è¯†åˆ«èšç±»ï¼Œæ‰£10åˆ†
        
        return max(0, coherence_score)
    
    def _is_concept_referenced(self, insight: str, reasoning: str) -> bool:
        """æ£€æŸ¥æ´å¯Ÿä¸­çš„æ¦‚å¿µæ˜¯å¦åœ¨æ¨ç†ä¸­è¢«å¼•ç”¨"""
        if not insight or not reasoning:
            return False
        
        # æå–æ´å¯Ÿä¸­çš„å…³é”®è¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        insight_keywords = set(insight.lower().split())
        reasoning_lower = reasoning.lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®è¯å‡ºç°åœ¨æ¨ç†ä¸­
        overlap = sum(1 for kw in insight_keywords if kw in reasoning_lower)
        
        # å¦‚æœæœ‰30%ä»¥ä¸Šçš„å…³é”®è¯é‡åˆï¼Œè®¤ä¸ºæœ‰å¼•ç”¨
        return overlap / len(insight_keywords) > 0.3 if insight_keywords else False
    
    # ========================================
    # è®¡ç®—æ–¹æ³•
    # ========================================
    
    def _insight_found(self, gt_insight: Dict, agent_insights: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦å‘ç°äº†æŸä¸ªground truthæ´å¯Ÿï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        
        æ³¨æ„ï¼šå»ºè®®ä½¿ç”¨ _calculate_insight_match_score
        """
        gt_content = gt_insight['content']
        
        # ç¼–ç ground truthæ´å¯Ÿ
        gt_embedding = self.semantic_model.encode(gt_content, convert_to_numpy=True)
        
        # éå†agentçš„æ´å¯Ÿï¼Œå¯»æ‰¾æœ€ç›¸ä¼¼çš„
        for agent_insight in agent_insights:
            if not agent_insight or len(agent_insight.strip()) < 5:
                continue
            
            # ç¼–ç agentæ´å¯Ÿ
            agent_embedding = self.semantic_model.encode(agent_insight, convert_to_numpy=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(gt_embedding, agent_embedding) / (
                np.linalg.norm(gt_embedding) * np.linalg.norm(agent_embedding) + 1e-8
            )
            
            # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š0.65ï¼ˆè¾ƒå®½æ¾ï¼Œå…è®¸ä¸åŒè¡¨è¾¾ï¼‰
            if similarity > 0.65:
                return True
        
        return False
    
    def _count_insights_found(self, explorations: List[Dict]) -> int:
        """ç»Ÿè®¡å‘ç°çš„æ´å¯Ÿæ•°é‡"""
        count = 0
        for exp in explorations:
            summary = exp.get('analysis_summary', {})
            count += len(summary.get('key_insights', []))
            count += len(summary.get('patterns_found', []))
        return count
    
    def _get_tools_used(self, explorations: List[Dict]) -> List[str]:
        """è·å–ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨"""
        tools = []
        for exp in explorations:
            tool_exec = exp.get('tool_execution') or {}
            if tool_exec:
                tool_name = tool_exec.get('tool_name')
                if tool_name:
                    tools.append(tool_name)
        return tools


def format_evaluation_report(eval_result: Dict, task_id: str) -> str:
    """æ ¼å¼åŒ–è¯„ä¼°æŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append(f"Benchmarkè¯„ä¼°æŠ¥å‘Š - {task_id}")
    report.append("=" * 60)
    report.append("")
    
    report.append(f"ğŸ“Š æ€»åˆ†: {eval_result['total_score']}/100")
    report.append("")
    
    report.append("ğŸ“ˆ å„ç»´åº¦å¾—åˆ†:")
    scores = eval_result['dimension_scores']
    weights = eval_result.get('weights', {'insight_quality': 0.60, 'reasoning_process': 0.40})
    report.append(f"  1. æ´å¯Ÿè´¨é‡ ({int(weights['insight_quality']*100)}%æƒé‡): {scores['insight_quality']}/100")
    report.append(f"     - Recall: å‘ç°äº†å¤šå°‘å…³é”®æ´å¯Ÿ")
    report.append(f"     - Precision: æ´å¯Ÿçš„å‡†ç¡®æ€§")
    report.append(f"     - Depth: æ´å¯Ÿçš„æ·±åº¦å±‚æ¬¡ï¼ˆ1=æè¿°ï¼Œ2=è¯Šæ–­ï¼Œ3=é¢„æµ‹ï¼‰")
    report.append("")
    
    report.append(f"  2. æ¨ç†è¿‡ç¨‹ ({int(weights['reasoning_process']*100)}%æƒé‡): {scores['reasoning_process']}/100")
    report.append(f"     - æ¨ç†è¿è´¯æ€§ (20%): æ­¥éª¤ä¹‹é—´æ˜¯å¦æœ‰é€»è¾‘è”ç³»")
    report.append(f"     - å·¥å…·è°ƒç”¨è¯„ä¼° (40%): æ˜¯å¦ä½¿ç”¨GTè¦æ±‚çš„å·¥å…·")
    report.append(f"     - å·¥å…·è·¯å¾„è¯„ä¼° (40%): è°ƒç”¨é¡ºåºæ˜¯å¦ä¸GTä¸€è‡´")
    report.append("")
    
    report.append("ğŸ“‹ æ¢ç´¢è¯¦æƒ…:")
    details = eval_result['details']
    report.append(f"  - æ¢ç´¢è½®æ¬¡: {details['total_explorations']}")
    report.append(f"  - å‘ç°æ´å¯Ÿ: {details['insights_found']}ä¸ª")
    report.append(f"  - ä½¿ç”¨å·¥å…·: {', '.join(details['tools_used']) if details['tools_used'] else 'æ— '}")
    report.append("")
    
    # è¯„çº§
    total_score = eval_result['total_score']
    if total_score >= 85:
        rating = "ğŸŒŸ ä¼˜ç§€ (Excellent)"
        comment = "å…¨é¢å®Œæˆä»»åŠ¡ï¼Œæ´å¯Ÿæ·±åˆ»ï¼Œæ¨ç†æ¸…æ™°ï¼Œæ•ˆç‡é«˜"
    elif total_score >= 70:
        rating = "âœ… è‰¯å¥½ (Good)"
        comment = "å®Œæˆä¸»è¦ä»»åŠ¡ï¼Œæ´å¯Ÿåˆç†ï¼Œæœ‰æ”¹è¿›ç©ºé—´"
    elif total_score >= 60:
        rating = "âš ï¸ åŠæ ¼ (Pass)"
        comment = "åŸºæœ¬å®Œæˆä»»åŠ¡ï¼Œä½†æ´å¯Ÿä¸å¤Ÿæ·±å…¥æˆ–è¿‡ç¨‹ä¸å¤Ÿä¼˜åŒ–"
    else:
        rating = "âŒ ä¸åŠæ ¼ (Fail)"
        comment = "æœªèƒ½æœ‰æ•ˆå®Œæˆä»»åŠ¡ï¼Œéœ€è¦é‡å¤§æ”¹è¿›"
    
    report.append(f"æ€»ä½“è¯„ä»·: {rating}")
    report.append(f"è¯„è¯­: {comment}")
    report.append("=" * 60)
    
    return "\n".join(report)


if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    print("Benchmarkè¯„ä¼°å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰å·²å°±ç»ª")
    print("\næ”¹è¿›ç‚¹ï¼š")
    print("âœ… 1. PrecisionçœŸå®è®¡ç®—ï¼ˆä¸å†å‡è®¾æ‰€æœ‰æ´å¯Ÿéƒ½æœ‰æ•ˆï¼‰")
    print("âœ… 2. DepthçœŸå®è¯„ä¼°ï¼ˆåŸºäºå…³é”®è¯æ£€æµ‹æ´å¯Ÿå±‚æ¬¡ï¼‰")
    print("âœ… 3. å†—ä½™æ£€æµ‹ç²¾ç¡®åŒ–ï¼ˆåŸºäºçŠ¶æ€æŒ‡çº¹è€Œéè¿ç»­é‡å¤ï¼‰")
    print("âœ… 4. é‡Œç¨‹ç¢‘æ£€æŸ¥å¢å¼ºï¼ˆåŒæ—¶æ£€æŸ¥å·¥å…·+æ´å¯Ÿï¼‰")
    print("âœ… 5. æ¨ç†è¿è´¯æ€§è¯„ä¼°ï¼ˆæ–°å¢ï¼‰")
    print("\næ‰€æœ‰åŸæœ‰åŠŸèƒ½ä¿æŒä¸å˜ï¼Œå¯æ— ç¼æ›¿æ¢åŸç‰ˆæœ¬ï¼")