"""
Benchmarkè¿è¡Œè„šæœ¬
åŠ è½½ä»»åŠ¡ â†’ æ‰§è¡Œç³»ç»Ÿ â†’ è¯„ä¼°æ‰“åˆ†
"""

import json
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from benchmark.evaluator import BenchmarkEvaluator, format_evaluation_report
from core.session_manager import SessionManager
from core.vega_service import get_vega_service
from core.modes.autonomous_exploration_mode import AutonomousExplorationMode


def load_benchmark_task(task_path: str) -> dict:
    """åŠ è½½benchmarkä»»åŠ¡"""
    with open(task_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_vega_spec(spec_path: str) -> dict:
    """åŠ è½½Vega-Lite spec"""
    with open(spec_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_benchmark_task(task_data: dict) -> dict:
    """æ‰§è¡Œbenchmarkä»»åŠ¡"""
    print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_data['task_id']}")
    print(f"   æŸ¥è¯¢: {task_data['task']['query']}")
    print()
    
    # åŠ è½½Vega spec
    spec_path = task_data['task']['initial_visualization']['vega_spec_path']
    vega_spec = load_vega_spec(spec_path)
    
    # åˆå§‹åŒ–æœåŠ¡
    vega_service = get_vega_service()
    session_mgr = SessionManager()
    
    # æ¸²æŸ“åˆå§‹è§†å›¾
    render_result = vega_service.render(vega_spec)
    if not render_result.get('success'):
        print(f"âŒ æ¸²æŸ“å¤±è´¥: {render_result.get('error')}")
        return None
    
    image_base64 = render_result['image_base64']
    
    # åˆ›å»ºsession
    chart_type = task_data['metadata']['chart_type']
    session_id = session_mgr.create_session(vega_spec, chart_type)
    session = session_mgr.get_session(session_id)
    
    # æ‰§è¡Œautonomous exploration
    mode = AutonomousExplorationMode()
    user_query = task_data['task']['query']
    
    context = session.get('context', {})
    
    result = mode.execute(
        user_query=user_query,
        vega_spec=vega_spec,
        image_base64=image_base64,
        chart_type=chart_type,
        context=context
    )
    
    print(f"âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
    print(f"   - æ¢ç´¢è½®æ¬¡: {result.get('total_iterations', 0)}")
    print(f"   - æ¨¡å¼: {result.get('mode', 'unknown')}")
    print()
    
    return result


def evaluate_result(task_data: dict, agent_result: dict) -> dict:
    """è¯„ä¼°ç»“æœ"""
    print("ğŸ“Š å¼€å§‹è¯„ä¼°...")
    
    ground_truth = task_data['ground_truth']
    evaluator = BenchmarkEvaluator(ground_truth)
    
    eval_result = evaluator.evaluate(agent_result)
    
    return eval_result


def save_results(task_id: str, agent_result: dict, eval_result: dict):
    """ä¿å­˜ç»“æœ"""
    output_dir = Path("benchmark/results")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # ä¿å­˜agentç»“æœ
    agent_file = output_dir / f"{task_id}_agent_result.json"
    with open(agent_file, 'w', encoding='utf-8') as f:
        json.dump(agent_result, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_file = output_dir / f"{task_id}_evaluation.json"
    with open(eval_file, 'w', encoding='utf-8') as f:
        json.dump(eval_result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¯ Interactive VA Benchmark - è½»é‡ç‰ˆ")
    print("=" * 60)
    
    # åŠ è½½ä»»åŠ¡
    task_path = "benchmark/tasks/scatter_clustering_001.json"
    print(f"\nğŸ“‚ åŠ è½½ä»»åŠ¡: {task_path}")
    
    try:
        task_data = load_benchmark_task(task_path)
        print(f"âœ… ä»»åŠ¡åŠ è½½æˆåŠŸ")
        print(f"   - ä»»åŠ¡ID: {task_data['task_id']}")
        print(f"   - éš¾åº¦: {task_data['metadata']['difficulty']}")
        print(f"   - äº¤äº’å¿…è¦æ€§: {task_data['metadata']['interaction_necessity']}")
    except Exception as e:
        print(f"âŒ ä»»åŠ¡åŠ è½½å¤±è´¥: {e}")
        return
    
    # æ‰§è¡Œä»»åŠ¡
    try:
        agent_result = run_benchmark_task(task_data)
        if not agent_result:
            print("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            return
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # è¯„ä¼°ç»“æœ
    try:
        eval_result = evaluate_result(task_data, agent_result)
        
        # æ‰“å°è¯„ä¼°æŠ¥å‘Š
        report = format_evaluation_report(eval_result, task_data['task_id'])
        print("\n" + report)
        
        # ä¿å­˜ç»“æœ
        save_results(task_data['task_id'], agent_result, eval_result)
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\nâœ¨ Benchmarkè¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()

