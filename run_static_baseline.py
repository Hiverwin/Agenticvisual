"""
é™æ€VLM Baselineæµ‹è¯•
ç›´æ¥ä½¿ç”¨DashScope APIæ ‡å‡†å½¢å¼è°ƒç”¨qwen-vl-plusï¼Œç®€å•åˆ†æå›¾è¡¨
"""

import json
import sys
import argparse
import os
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥DashScope
try:
    import dashscope
    from dashscope import MultiModalConversation
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    print("âŒ é”™è¯¯: æœªå®‰è£…dashscopeåº“")
    print("   è¯·è¿è¡Œ: pip install dashscope")
    sys.exit(1)

from core.vega_service import get_vega_service


def load_benchmark_task(task_path: str) -> dict:
    """åŠ è½½benchmarkä»»åŠ¡"""
    task_file = Path(task_path)
    if not task_file.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {task_path}")
        sys.exit(1)
    
    with open(task_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_vega_spec(spec_path: str) -> dict:
    """åŠ è½½Vega-Lite spec"""
    spec_file = Path(spec_path)
    if not spec_file.exists():
        print(f"âŒ é”™è¯¯: Vega specæ–‡ä»¶ä¸å­˜åœ¨: {spec_path}")
        sys.exit(1)
    
    with open(spec_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_static_baseline(task_data: dict) -> dict:
    """è¿è¡Œé™æ€baselineåˆ†æï¼ˆç›´æ¥è°ƒç”¨DashScope APIï¼‰"""
    print("=" * 60)
    print("ğŸ”¬ é™æ€VLM Baselineæµ‹è¯•")
    print("=" * 60)
    print()
    print(f"ä»»åŠ¡ID: {task_data['task_id']}")
    print(f"æ¨¡å¼: é™æ€åˆ†æï¼ˆæ— äº¤äº’ï¼‰")
    print()
    
    # æ£€æŸ¥API Key
    api_key = os.getenv('DASHSCOPE_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    dashscope.api_key = api_key
    
    # åŠ è½½Vega specå¹¶æ¸²æŸ“
    spec_path = task_data['task']['initial_visualization']['vega_spec_path']
    vega_spec = load_vega_spec(spec_path)
    
    vega_service = get_vega_service()
    render_result = vega_service.render(vega_spec)
    
    if not render_result.get('success'):
        print(f"âŒ æ¸²æŸ“å¤±è´¥: {render_result.get('error')}")
        return None
    
    image_base64 = render_result['image_base64']
    user_query = task_data['task']['query']
    
    print(f"ğŸ“Š åˆå§‹è§†å›¾å·²æ¸²æŸ“")
    print(f"â“ æŸ¥è¯¢: {user_query}")
    print()
    
    # ç›´æ¥ä½¿ç”¨DashScope APIæ ‡å‡†å½¢å¼è°ƒç”¨
    print("ğŸ¤– æ­£åœ¨è°ƒç”¨DashScope API (qwen-vl-plus)...")
    
    try:
        response = MultiModalConversation.call(
            model='qwen-vl-plus',
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"text": user_query},
                        {"image": f"data:image/png;base64,{image_base64}"}
                    ]
                }
            ]
        )
        
        if response.status_code == 200:
            vlm_output = response.output.choices[0].message.content[0]["text"]
            print("âœ… APIè°ƒç”¨æˆåŠŸ")
            
            print()
            print("=" * 60)
            print("VLMè¾“å‡º:")
            print("=" * 60)
            print(vlm_output[:500] + "..." if len(vlm_output) > 500 else vlm_output)
            print("=" * 60)
            print()
            
            # æ„å»ºç»“æœæ ¼å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼Œç”¨äºåç»­è¯„ä¼°ï¼‰
            result = {
                "session_id": f"static_baseline_{task_data['task_id']}",
                "timestamp": datetime.now().isoformat(),
                "mode": "static_vlm_baseline",
                "total_iterations": 1,
                "explorations": [
                    {
                        "iteration": 1,
                        "success": True,
                        "timestamp": 0,
                        "vlm_raw_output": vlm_output,
                        "images": [image_base64],
                        "analysis_summary": {
                            "key_insights": [],  # ç®€åŒ–ï¼šä¸è§£æï¼Œä¿æŒç©ºåˆ—è¡¨
                            "patterns_found": [],
                            "anomalies": [],
                            "recommendations": []
                        },
                        "tool_execution": None,
                        "duration": 0
                    }
                ]
            }
            
            return result
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: status={response.status_code}, message={response.message}")
            return None
            
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def save_result(task_id: str, result: dict):
    """ä¿å­˜ç»“æœ"""
    output_dir = Path("benchmark/results")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    output_file = output_dir / f"{task_id}_static_baseline.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    return output_file


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='è¿è¡Œé™æ€VLM baselineæµ‹è¯•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python run_static_baseline.py benchmark/tasks/scatter_clustering_001.json
  python run_static_baseline.py benchmark/tasks/cars_multivariate_002.json
        """
    )
    parser.add_argument(
        'benchmark_path',
        help='benchmarkä»»åŠ¡JSONæ–‡ä»¶è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    print("\nğŸ“‚ åŠ è½½ä»»åŠ¡...")
    task_data = load_benchmark_task(args.benchmark_path)
    
    print("ğŸš€ å¼€å§‹é™æ€VLMåˆ†æ...\n")
    result = run_static_baseline(task_data)
    
    if result:
        output_file = save_result(task_data['task_id'], result)
        
        print("\nâœ¨ é™æ€baselineæµ‹è¯•å®Œæˆï¼")
        print(f"\nä¸‹ä¸€æ­¥: è¿è¡Œè¯„ä¼°å™¨è¯„ä¼°ç»“æœ")
        print(f"   python test_benchmark.py {output_file}")
    else:
        print("\nâŒ é™æ€baselineæµ‹è¯•å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
